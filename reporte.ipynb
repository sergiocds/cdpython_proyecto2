{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2\n",
    "---\n",
    "#### Ingeniería de Datos con Python  \n",
    "Universidad Galileo  \n",
    "Instituto de Investigación de Operaciones  \n",
    "Master en Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Integrantes del grupo*  \n",
    "**Peña Maltez, Andres Alberto - carnet 23004061**  \n",
    "**Castillo Custodio, Sergio Josué - carnet 23000331**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducción**\n",
    "La ingeniería de datos es un campo dentro de la ingeniería de software dedicada a la construcción de sistemas que permiten la colección y uso de datos, que generalmente serán utilizados en tareas de análisis y ciencia de datos. Hacer que los datos sean utilizables para estas tareas generalmente ímplican tareas de procesamiento y limpieza de datos que consumen recursos considerables de cómputo y almacenamiento.\n",
    "\n",
    "Una de las tareas más importantes de la ingeniería de datos es la creación de *pipelines* ETL para cantidades másivas de datos (*big data*), con los cuales se gestiona el flujo de datos a lo largo de la organización, posibilitando así tomar cantidades inmensas de datos y convertirlos en conocimiento. Estos gestión debe tomar en consideración que los datos deben estar disponibles con rápidez, conservando su integridad y facilitando resiliencia, escalabilidad y seguridad.\n",
    "\n",
    "El presente proyecto consiste en el desarrollo de un pipeline de ingeniería de datos utilizando Python, SQL y los servicios de RDS y S3 de AWS para los datos de los servicios prestados por una agencia de viajes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scope**\n",
    "El objetivo del proyecto es desarrollar un pipeline ETL utilizando Python, SQL y los servicios RDS y S3 de AWS que tome los datos de una base de datos de una agencia de viajes, los extraíga, prepare e inserte los datos en un Data Warehouse para su posterior análisis.\n",
    "\n",
    "La agencia de viajes a analizar opera en Brasil y se consideran los servicios prestados un grupo seleccionado de empresas a las cuales se les venden servicios de vuelos y estadías de hotel para el periodo comprendido entre el 26 de septiembre de 2019 y el 24 de julio de 2023. Todos los datos a analizar (dataset) son suministrados a través de tres archivos CSV:\n",
    "- Uno con datos de usuarios (viajeros)\n",
    "- Uno con datos de estadias de hotel\n",
    "- Uno con datos de vuelos\n",
    "\n",
    "Puede descargar el dataset haciendo [click aquí](https://www.kaggle.com/datasets/leomauro/argodatathon2019?select=flights.csv\n",
    "\"Travel Dataset\").\n",
    "\n",
    "Los recursos disponibles para el proyecto son:\n",
    "- *Python*\n",
    "- *SQL*, utilizando como gestor *MySQL*\n",
    "- Servicios *RDS* y *S3* de *AWS*\n",
    "- *Tableau Desktop*\n",
    "- Repositorio en *GitHub* con los archivos del proyecto ([click aquí](https://github.com/sergiocds/cdpython_proyecto2\n",
    "\"GitHub Repo\")).\n",
    "\n",
    "Los entregables son:\n",
    "- Reporte del proyecto (presente documento)\n",
    "- Base de datos relacional alojada en *AWS RDS*\n",
    "- Base de datos dimensional alojada en *AWS RDS*\n",
    "- Scripts de Python con procedimientos ETL.\n",
    "- Dashboard para análisis de datos elaborado con *Tableau Desktop*\n",
    "\n",
    "Queda fuera del alcance del proyecto:\n",
    "- Implementar el data warehouse en el servicio *Redshift* de AWS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objetivos**\n",
    "**Objetivo Principal**  \n",
    "Desarrollar un pipeline ETL utilizando Python, SQL y los servicios RDS y S3 de AWS que tome los datos de una base de datos de una agencia de viajes, los extraíga, prepare e inserte los datos en un Data Warehouse para su posterior análisis.\n",
    "  \n",
    "**Objetivos Secundarios**  \n",
    "1. A partir del dataset de la agencia de viajes, construir un modelo relacional para una base de datos normalizada en *MySQL* de los datos originales provistos.\n",
    "2. Con la base de datos construida para la agencia de viajes, construir un modelo dimensional para un *Data Warehouse* en *MySQL*, que permita tareas de análisis y elaboración de reportes.\n",
    "3. Almacenar la base de datos y el *data warehouse* para los datos de la agencia de viajes en *RDS* de *AWS*.\n",
    "4. Desarrollar con *Python* los scripts que contengan los procedimientos para la extración, preparación y carga de datos, tanto en la base de datos como en el *data warehouse*.\n",
    "5. Análizar los datos para responder a las siguientes preguntas sobre la operación de la agencia de viajes:\n",
    "    - ¿Cuál es el principal cliente de la agencia de viajes?\n",
    "    - ¿Cuál es la ciudad más visitada por los clientes de la agencia de viajes?\n",
    "    - ¿Cuál es el trimestre del año con más viajes agendados en la agencia de viajes?\n",
    "    - ¿Cuál es el hotel por ciudad más utilizado?\n",
    "    - ¿Cuales son los 10 viajeros más frecuentes de la agencia de viajes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploración**\n",
    "El dataset, conformado por los tres archivos CSV contiene datos no normalizados de los servicios prestados por la agencia de viajes.\n",
    "\n",
    "*Users*  \n",
    "Contiene datos para 1 340 usuarios de la agencia de viajes (viajeros). Los datos almacenados son:\n",
    "- Code: código de usuario\n",
    "- Company: compañía para la que labora el usuario\n",
    "- Name: nombre del usuario\n",
    "- Gender: sexo del usuario\n",
    "- Age: edad del usuario\n",
    "\n",
    "*Hotels*  \n",
    "Contiene datos de 40 552 estadías de hotel para los usuarios de la agencia de viaje. Estas estadías son en 9 hoteles distintos ubicados en ciudades diferentes de Brasil. Los datos almacenados son:\n",
    "- travelCode: código identificador del viaje al que está asociada la estadía en el hotel\n",
    "- userCode: código identificador del usuario que realiza el viaje/estadía de hotel\n",
    "- name: nombre del hotel\n",
    "- place: ciudad donde está ubicado el hotel\n",
    "- days: número de días de la estadía en el hotel\n",
    "- price: precio por noche de estadía en el hotel\n",
    "- total: precio total de la estadía en el hotel\n",
    "- date: fecha del chequeo (check-in) en el hotel\n",
    "\n",
    "*Flights*  \n",
    "Contiene datos de 271,888 pasajes de vuelo para los usuarios de la agencia de viaje, entre distintas ciudades de Brasil. Los datos almacenados son:\n",
    "- travelCode: código identificador del viaje al que está asociada la estadía en el hotel\n",
    "- userCode: código identificador del usuario que realiza el viaje/estadía de hotel\n",
    "- from: aeropuerto de origen del vuelo\n",
    "- to: aeropuerto de destino del vuelo\n",
    "- flightType: tipo de vuelo\n",
    "- price: precio del vuelo\n",
    "- time: duración en horas del vuelo\n",
    "- distance: distancia del recorrido del vuelo\n",
    "- agency: aerolínea del vuelo\n",
    "- date: fecha de salida del vuelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Datos\n",
    "El dataset provisto no se encuentra normalizado. Los datos están dividos en tres archivos CSV distintos, los cuales ya se describieron anteriormente. Teniendo en mente que la finalidad del proyecto es la construcción de un *data warehouse*, el primer paso es trasladar el dataset a una base de datos relacional.\n",
    "\n",
    "Una vez los datos han sido cargados en esta base de datos, estos se cargan en el *data warehouse*.\n",
    "\n",
    "Como gestor, tanto para la base de datos como para el *data warehouse*, se utiliza *MySQL*.\n",
    "\n",
    "Ambas bases de datos se almacenaron en instancias separadas de *RDS* de *AWS*, las cuales se crearon desde Python utilizando la librería boto3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Relacional\n",
    "El modelo relacional se muestra en la siguiente figura. El código DDL de MySQL se encuentra en el archivo *create_travels_DB.py* del repositorio de [GitHub](https://github.com/sergiocds/cdpython_proyecto2\n",
    "\"GitHub Repo\").\n",
    "\n",
    "*Figura 1: Modelo relacional*\n",
    "![Modelo Relacional](https://github.com/sergiocds/cdpython_proyecto2/blob/main/modelo_relacional.png?raw=true)  \n",
    "*Fuente: elaboración propia.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Dimensional\n",
    "El modelo dimensional se muestra en la siguiente figura. El código DDL de MySQL se encuentra en el archivo *create_travels_DW.py* del repositorio de [GitHub](https://github.com/sergiocds/cdpython_proyecto2\n",
    "\"GitHub Repo\").\n",
    "\n",
    "El modelo dimensional utilizó dos tablas de hechos, una para vuelos y otra para estadías de hotel, en las que la granularidad es a nivel de cada vuelo individual y de cada estadía de hotel individual, respectivamente. Adicionalmente, se creo una tabla de hechos de snapshot, con granularidad a nivel de cada viaje individual en la que se sumarizan las estadías de hotel y vuelos asociadas a un viaje determinado. Se tomo la decisión de crear múltiples tablas de hechos pues había incompatibilidad de granularidad entre vuelos, estadías de hotel y viajes. Y para no simplificar excesivamente el análisis, se optó por el modelo presentado.\n",
    "\n",
    "*Figura 2: Modelo dimensional*\n",
    "![Modelo Relacional](https://github.com/sergiocds/cdpython_proyecto2/blob/main/modelo_dimensional.png?raw=true)  \n",
    "*Fuente: elaboración propia.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento\n",
    "Se crearon dos scripts de Python, almacenados en el repositorio de [GitHub](https://github.com/sergiocds/cdpython_proyecto2\n",
    "\"GitHub Repo\") del proyecto.\n",
    "\n",
    "El primero de ellos, DBtravel.py, contiene todo el código necesario para extraer los datos de los archivos CSV y modificarlos de modo que puedan ser insertados en la base de datos correspondiente.\n",
    "\n",
    "El segundo de ellos, DWtravel.py, contiene el código requerido para leer los datos de la base de datos relacional y cargarla en DataFrames de pandas. Una vez cargados los DataFrames, se hacen las modificaciones necesarias de modo que se puedan crear las tablas de dimensiones y hechos y por último puedan ser cargadas en el *data warehouse*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analítica\n",
    "Para la parte de analítica, los lineamientos del proyecto no establecieron cual debía ser el procedimiento a seguir o cual debía ser la herramienta a utilizar. Por lo que con el *data warehouse* creado, se cargo a un *workbook* de *Tableau Desktop*. Dentro de este se replicó el modelo dimensional, estableciendo las relaciones correspondientes. Seguidamente se crearon las vistas y el dashboard necesarios para analizar los datos y responder a las preguntas planteadas como parte de los objetivos:\n",
    "\n",
    "- ¿Cuál es el principal cliente de la agencia de viajes?  \n",
    "    **Respuesta:**  \n",
    "    Tanto en términos de vuelos como de estadías de hotel, el principal cliente de la agencia de viajes es la compañía 4You, con un gasto en vuelos de 86,243,818.00 y en estadías de hotel de 7,486,217.00 para el periodo comprendido entre el 26/septiembre/2019 y el 24/julio/2023.\n",
    "\n",
    "- ¿Cuál es la ciudad más visitada por los clientes de la agencia de viajes?  \n",
    "    **Respuesta:**  \n",
    "    La ciudad más visitada por los clientes de la agencia de viajes es Florianopolis(SC), con un total de vuelos hacia esta ciudad de 57,317.\n",
    "\n",
    "- ¿Cuál es el trimestre del año con más viajes agendados en la agencia de viajes?  \n",
    "    **Respuesta:**  \n",
    "    El volumen de viajes es practicamente constante a lo largo de todo el año con cerca de 40,000 viajes por trimestre a lo largo de todo el periodo de tiempo analizado. Cabe resaltar que para el año 2019 no se cuentan con datos practicamente de los primeros 3 trimestres.\n",
    "\n",
    "    Analizando por separado cada año, se observá que el primer trimestre de cada uno de ellos es en el que se realiza el mayor número de viajes. Como observacion adicional, se puede notar que año con año, la tendencia en el número de viajes ha sido decreciente. El efecto de las restricciones de viajes por la pandemia por COVID-19 podría explicar esta disminución.\n",
    "\n",
    "- ¿Cuál es el hotel por ciudad más utilizado?  \n",
    "    **Respuesta:**  \n",
    "    El hotel más utilizado para el periodo de tiempo analizado es el hotel K, en la ciudad de Salvador(BH) con un total de 5,094 estadías.\n",
    "\n",
    "- ¿Cuales son los 10 viajeros más frecuentes de la agencia de viajes?  \n",
    "    **Respuesta:**  \n",
    "    Los diez usuarios o viajeros más frecuentes de la agencia de viajes para el periodo de tiempo analizado son:\n",
    "\n",
    "    |Company |Name|Number of travels|\n",
    "    |:-----|:----:|----:|\n",
    "    |4You |Mark Eisentrout |200 |\n",
    "    |4You |Mary Ewers |200 |\n",
    "    |4You |Sonia Malaspina |200 |\n",
    "    |4You |Wallace Gallardo |200 |\n",
    "    |Acme Factory |Forrest Kaufman |200 |\n",
    "    |Acme Factory |Jessie Armstrong |200 |\n",
    "    |Acme Factory |Ted Bonneau |200 |\n",
    "    |Monsters CYA |Nathan Ponder |200 |\n",
    "    |Umbrella LTDA |Helen Warner |199 |\n",
    "    |Wonka Company |Anthony Young |200 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "1. La integración de *Python*, *SQL* y *AWS* representan una herramienta poderosa y altamente automatizable para la creación y gestión de pipelines ETL, permitiendo el almacenamiento cloud de todos los datos.\n",
    "2. El uso de *Python* para la gestión de datos, especialmente a través de la librería *pandas* permite la manipulación de grandes cantidades de datos de forma muy eficiente y rápida, gracias a los algoritmos específicamente diseñados para este propósito y a la amplía variedad de métodos y funciones con los que cuenta la libería.\n",
    "3. A través de librerías como *boto3*, es posible integrar *Python* y *AWS* de forma muy sencilla, simplificando la creación de instancias directamente a través de código."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_proyecto2cdpds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
